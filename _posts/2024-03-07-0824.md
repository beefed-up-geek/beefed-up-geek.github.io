---
layout: single
title: "Algorithm 2: Algorithm, induction, time complexity, RAM model"
categories: Algorithm
tags: [Algorithm, Maximum subarray sum]
---

# Algorithm, induction, efficiency evaluation, and RAM model

reference: MIT 6.006 Introduction to Algorithms

{% include video id="ZA-tUyM_y7s" provider="youtube" %}

This lecture mainly deals with main concepts of algorithms, induction, time complexity and RAM model. 

Lets delve into what they are.

## Algorithm

What algorithm basically is a **plan** to solve a computational problem.  But it has to be more **abstract** than a actual code, yet **formal** in a sense of computational flowchart. 

For example, let's say we were to find students with same birthday in the lecture hall. The algorithm to solve such a problem would be the following.

> Initialize a list to record birthdays
>
> check each student's birthdays for every student in classroom
>
> ​	check the students birthday
>
> ​	if the birthday is on the record return those two students
>
> ​	if not add it to record
>
> after checking all students without finding a duplicate,  conclude that there is no match



## Induction

Proof of correctness is important in algorithm analysis. **Induction** is the most common way to prove. Induction requires 3 components.

### 3 Components of induction

1. #### Base case

   It is a **simplest case for which our proposition is *true***. This case becomes the foundation of our induction.

2. #### Inductive hypothesis

   This hypothesis is a assumption that our statement is true for some arbitrary case. For instance, "Let's assume that **the statement is true for some k**".

3. #### Inductive step

   In this step we use the Inductive hypothesis to prove that **our statement holds for the cases of K+1**. If our statement is proven to be true in cases of k+1 when k is *true*, our statement is self evidently *true* since we have a base case. 

To prove the correctness of the birthday algorithm using induction, we follow these steps, as outlined in the provided explanation:

### Proving birthday algorithm with induction 

1. #### Base case

   The simplest scenario for this algorithm is when zero students have been interviewed. At this point, no work has been done, and it's obvious that no pair of students with the same birthday could have been found because no birthdays have been recorded yet. This establishes the base case, showing that the algorithm correctly identifies there are no matches when no data is present.

2. #### Inductive hypothesis

   Assume the algorithm works correctly after interviewing *k* students. This means if any two students out of the first *k* share a birthday, the algorithm has already identified and reported this pair. If not, it has accurately recorded the birthdays without finding any match.

3. #### Inductive step

   Now, consider the *k*+1-th student. There are two scenarios:

   1. **If a match was found among the first *k* students**: By our inductive hypothesis, the algorithm has already reported this, so the algorithm continues to hold true.
   2. **If no match was found among the first *k* students**: When the *k*+1-th student is interviewed, the algorithm checks this new birthday against all previously recorded birthdays. If a match is found, the algorithm reports it, confirming the inductive hypothesis. If no match is found, the birthday is added to the record, maintaining the algorithm's correctness.

## Efficiency evaluation	

​	Most important aspect of efficiency when it comes to algorithm is a **cost of time**. However, actual execution time inevitably varies by strength of our computer, so it is unsuitable to evaluate an algorithm based on actual time cost. For instance, even if we had the same algorithm or furthermore the exact code, time cost difference between the IBM research computer and a simple calculator exists. 

​	Then how can we evaluate the efficiency? 

1. ### Counting Fundamental Operations

   Rather than measuring time directly, the efficiency of an algorithm is often assessed by counting the number of fundamental operations it performs. These operations are considered to take a fixed amount of time and include basic actions such as comparisons, arithmetic operations, and memory access. The idea is to abstract away from the specifics of any particular hardware and provide a more universally applicable measure of the algorithm's performance.

2. ### Asymptotic Analysis

    This is the primary method used to evaluate algorithm efficiency. It involves analyzing the algorithm's performance in terms of its input size, using notations such as Big O (upper bound), Omega (lower bound), and Theta (tight bound). Asymptotic analysis allows for the comparison of algorithms based on their growth rates, i.e., how the time (or space) requirement grows as the size of the input increases.

3. ### Input Size Consideration

   The size of the input, often denoted as *n*, is a crucial factor in determining an algorithm's efficiency. The performance is generally expected to vary with the size of the input. For instance, sorting a small array may be quick even with a simple algorithm, but as the size of the array grows, more sophisticated algorithms that grow more slowly with n* prove to be more efficient.

​	

In summary, evaluating an algorithm's efficiency requires an understanding of **asymptotic behavior**, **counting fundamental operations** instead of **real-time execution**, and **considering the impact of input size** on performance. These measures provide a basis for comparing algorithms in a way that is independent of specific hardware and implementation details.



## RAM model

The Random Access Machine (RAM) model is used in theoretical computer science to abstract the idea of computation, allowing us to evaluate the efficiency of algorithms without getting bogged down by hardware-specific details. Here's a summary based on the provided explanation:

1. ### Random Access Memory (RAM): 

   The RAM model assumes that the computer has memory that can be accessed randomly in constant time. This means any memory cell can be read from or written to in a fixed amount of time, regardless of its position or the size of the memory.

2. ### Word RAM

   The model is further refined to the Word RAM model, where 'word' refers to the standard size of data with which the machine operates. Modern computers typically use word sizes of 32 or 64 bits. The word size dictates how much data can be processed at once and influences the machine's addressing capability.

3. ### Operations

   In the Word RAM model, the computer can perform a set of fundamental operations on words in memory. These operations include arithmetic (addition, subtraction, multiplication, division), comparison, and logical operations (AND, OR, NOT), all assumed to be done in constant time.

4. ### Addressing Memory

   Memory is addressed in chunks known as 'words,' and the CPU can fetch a word from memory, perform operations on it, and store it back in memory, all in constant time. This abstraction allows us to focus on the logic of algorithms without worrying about the physical memory layout or the specifics of memory access times.

5. ### Implications for Algorithm Analysis

   By abstracting the details of how data is stored and accessed, the RAM model allows us to focus on the number of operations an algorithm performs relative to the input size. This is essential for asymptotic analysis, where we're interested in understanding how the runtime of an algorithm scales with the size of its input, using notations like Big O, Omega, and Theta.

6. ### Limitations on Memory and Addressing

   While the RAM model assumes constant-time access to memory, it also acknowledges physical limitations, such as the maximum addressable memory space, which is determined by the word size. For instance, a 32-bit system can address up to 4 GB of memory, whereas a 64-bit system significantly expands this limit.

In summary, the RAM model provides a simplified but powerful framework for analyzing the efficiency of algorithms, ignoring the complexities of real-world hardware while still offering insights that are applicable to practical computing scenarios.

